# Deep Learning Course Notebooks

This repository contains my Jupyter Notebook implementations and assignments from the **Deep Learning** course. The materials cover fundamental and advanced topics such as neural networks, convolutional networks, transformers, generative models, and recent trends in self-supervised and foundation models.

Each session includes notebooks with code experiments, implementations of algorithms, and analyses.

---

## üìö Course Topics

| Session | Topic |
|:---:|:---|
| 1 | Introduction to Deep Learning & Machine Learning Review |
| 2 | MLP I ‚Äì Neural Networks, Universal Approximation Theorem |
| 3 | MLP II ‚Äì Training Neural Networks & Back-Propagation |
| 4 | Optimization |
| 5 | Optimization & Generalization |
| 6 | Training Techniques |
| 7 | Convolutional Neural Networks (CNN) |
| 8 | CNN Architectures |
| 9 | CNNs in Vision Problems (Segmentation, Object Detection, etc.) |
| 10 | RNN I ‚Äì RNNs and LSTMs |
| 11 | RNN II ‚Äì Word Embeddings, Language Modeling |
| 12 | Attention & Transformer I |
| 13 | Transformer II ‚Äì BERT, T5, GPT, ViT |
| 14 | Transformer III ‚Äì Large Language Models |
| 15 | Generative Models I ‚Äì Variational Autoencoders (VAE) |
| 16 | Generative Models II ‚Äì Generative Adversarial Networks (GAN) |
| 17 | Generative Models III ‚Äì Diffusion Models |
| 18 | Generative Models IV ‚Äì Diffusion Models (Continued) |
| 19 | Self-supervised Learning I ‚Äì Pretext Tasks, Contrastive Learning |
| 20 | Self-supervised Learning II ‚Äì Losses and Architectures |
| 21 | Recent Foundation Models ‚Äì CLIP, DALL¬∑E |
| 22 | Graph Neural Networks |
| 23 | Visualization & Interpretability |
| 24 | Adversarial Robustness |
| 25 | Advanced Topics |

---

## üìù Homeworks and Projects

Below you will find the links to each session‚Äôs notebooks with a brief description of their content.

---

### HW 1 ‚Äì Foundations of Deep Learning

- [Notebook 1: Optimization Algorithms](HW1/HW1_PART1.ipynb)  
  Implementation of optimization algorithms including Gradient Descent, Momentum, RMSProp, Adam, and the Newton Method.
- [Notebook 2: Implementing an MLP from scratch](HW1/Neural_Networks_from_scratch_with_numpy.ipynb)  
  Development of a multi-layer perceptron entirely in NumPy, evaluated on the MNIST and California Housing datasets.
- [Notebook 3: PyTorch Tutorial](HW1/pytorch_basic.ipynb)  
  Introduction to PyTorch tensors and their practical challenges. Includes the implementation of a simple MLP using PyTorch for the MNIST dataset.
---



